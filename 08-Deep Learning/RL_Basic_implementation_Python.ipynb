{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Basic_implementation_Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7Yol2WzMYceSg7Tr2luss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/AI-Engineering/blob/master/08-Deep%20Learning/RL_Basic_implementation_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iMpq6SrE9Po"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create Environment class\n",
        "class MyEnvironment:\n",
        "\n",
        "  def __init__(self):\n",
        "    #maximum number of steps which the agent can take to gain rewards\n",
        "    self.remaining_steps=20 #assume that the game must be completed within 20 steps\n",
        "\n",
        "  def get_observation(self):\n",
        "    #it can be any number of coordinates.Its considered as 3 here.\n",
        "    #These values -0.0,0.0,0.0 represent some kind of logic that gives info about the environment.These values can be anything.\n",
        "    return [1.0,2.0,1.0]  \n",
        "  \n",
        "  #when agent,performs an action,it should get a reward\n",
        "  #i have set it as 1 for reward,-1 for punishment\n",
        "  def get_actions(self):\n",
        "    return [-1,1]\n",
        "\n",
        "  #if steps are completed,return True because the agent should not move anymore\n",
        "  def check_is_done(self)->bool:\n",
        "    return self.remaining_steps==0\n",
        "\n",
        "  def action(self,int):\n",
        "    if self.check_is_done():\n",
        "      raise Exception(\"Game over\")      \n",
        "    self.remaining_steps-=1  #if steps can still be taken-game not finished=>decrement the remaining number of steps\n",
        "    return random.random()  #here-as this is a simple implementation-just returning a random number"
      ],
      "metadata": {
        "id": "fGwVELy2FADL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#agent implements some policy\n",
        "\n",
        "class myAgent:\n",
        "  def __init__(self):\n",
        "    self.total_rewards=0.0 #initially-agent-no rewards\n",
        "\n",
        "  def step(self,ob:MyEnvironment):\n",
        "    curr_obs=ob.get_observation()\n",
        "    print(curr_obs)\n",
        "    curr_action=ob.get_actions()\n",
        "    print(curr_action)\n",
        "    curr_reward=ob.action(random.choice(curr_action)) \n",
        "    #here,we are randomly picking -1 or 1\n",
        "    #usually,when action() is invoked,implementation to check if the decision of the agent is crt-give positive reward else negative reward\n",
        "    self.total_rewards+=curr_reward\n",
        "    print(\"Total rewards so far= %.3f \"%self.total_rewards)"
      ],
      "metadata": {
        "id": "VAE_74xZFDMS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  obj=MyEnvironment()\n",
        "  agent=myAgent()\n",
        "  step_number=0\n",
        "\n",
        "  while not obj.check_is_done():\n",
        "    step_number+=1\n",
        "    agent.step(obj)\n",
        "    \n",
        "\n",
        "  print(\"Total reward is %.3f \"%agent.total_rewards)\n",
        "  #different o/p everytime we run this code b'coz diff random numbers will be generated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ehv281FF-7",
        "outputId": "ea1123af-e8b7-4e31-9c80-c0c061a18c7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 0.521 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 0.729 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 1.388 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 1.395 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 1.622 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 1.679 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 1.865 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 2.134 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 2.147 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 2.621 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 3.169 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 3.336 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 3.505 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 3.828 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 4.081 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 5.078 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 5.102 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 5.917 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 6.413 \n",
            "[1.0, 2.0, 1.0]\n",
            "[-1, 1]\n",
            "Total rewards so far= 6.600 \n",
            "Total reward is 6.600 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "htDAvgZOFIu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}