{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_streaming_triggering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqPIm3kJk4xRgTCXKfDirG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/AI-Engineering/blob/master/13-PySpark/spark_streaming_triggering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2YWJVtO87sj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"spark_streaming\").config(\"spark.sql.shuffle.partitions\",3)\\\n",
        "        .config(\"spark.streaming.stopGracefullyOnShutdown\",True).getOrCreate()\n",
        "\n",
        "    df_ = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"9093\").load()\n",
        "\n",
        "    df1 = df_.selectExpr(\"explode(split(value,' ')) as word\")\n",
        "    df2 = df1.groupBy(\"word\").count()\n",
        "\n",
        "    result = df2.writeStream.format(\"console\").outputMode(\"complete\").option(\"checkpointLocation\",\"checkppoint-location1\")\\\n",
        "        .trigger(processingTime=\"30 seconds\").start()\n",
        "    result.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"spark_streaming\").config(\"spark.sql.shuffle.partitions\",3)\\\n",
        "        .getOrCreate()\n",
        "\n",
        "    df_ = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"9092\").load()\n",
        "\n",
        "    #split will always give array\n",
        "    #Explode on the array to get each words in different row.\n",
        "\n",
        "    df1 = df_.selectExpr(\"explode(split(value,' ')) as word\")\n",
        "\n",
        "    df2 = df1.groupBy(\"word\").count()\n",
        "\n",
        "    ##write to console\n",
        "    result = df2.writeStream.format(\"console\").outputMode(\"complete\")\\\n",
        "        .option(\"checkpointLocation\",\"checkppoint-location1\")\\\n",
        "        .start()\n",
        "\n",
        "\n",
        "    result.awaitTermination()"
      ],
      "metadata": {
        "id": "d1oPeZ3h9EEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    spark =SparkSession.builder.appName(\"spark_streaming\").getOrCreate()\n",
        "\n",
        "\n",
        "    ##read the stream\n",
        "\n",
        "    df_ = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"9092\").load()\n",
        "\n",
        "    #Print the schema\n",
        "    df_.printSchema()\n",
        "\n",
        "    ##write to sink\n",
        "    result = df_.writeStream.format(\"console\").outputMode(\"append\").start()\n",
        "\n",
        "\n",
        "    result.awaitTermination()"
      ],
      "metadata": {
        "id": "1X-m30JI9IWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}